{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQic7KJRaSP6",
        "outputId": "43480aef-ef11-4fea-9ade-b7c458cba9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ6fA-pdbIfO",
        "outputId": "5787b08d-64f9-4145-922c-7649111861c3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'thanhkieu (Python 3.9.21)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n thanhkieu ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_sg5eULsbh2Q",
        "outputId": "1f7f1be2-d36c-435b-afc2-5ad7752d4450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorFlow==2.17.0 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorFlow==2.17.0\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorFlow==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrN-6EKjJhvU",
        "outputId": "5f8b85ad-5cf5-4662-c026-1fecf77bfda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from pyvi) (1.5.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->pyvi) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->pyvi) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from sklearn-crfsuite->pyvi) (4.64.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m193.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-macosx_11_0_arm64.whl (319 kB)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TFX7wSAmg9y"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'thanhkieu (Python 3.9.21)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n thanhkieu ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import tensorflow as tf\n",
        "from pyvi import ViTokenizer\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuW_FhavcXNb",
        "outputId": "bbb0eb85-21cc-46dd-892a-de4e6983c7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.23.5)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a7lMy03omg93",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_train = pd.read_csv(\"/content/drive/MyDrive/NLP/vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/NLP/vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HR1jAzImg94",
        "outputId": "09362663-1689-4ac5-a54e-c36c08aea3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1050, 2)\n",
            "(5100, 2)\n"
          ]
        }
      ],
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jvrbwPfZmg95"
      },
      "outputs": [],
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3HlbVeHimg95"
      },
      "outputs": [],
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lm4OCwxXmg96"
      },
      "outputs": [],
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = []\n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nW2OZgkgmg97"
      },
      "outputs": [],
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pTb0MeDRmg98"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jW-7mKtWmg9-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-BHpPSLTmg9_"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LlV3M2dimg9_"
      },
      "outputs": [],
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dl9VZ3Rmg-A",
        "outputId": "da2bc066-1096-45a5-c604-2e84d937d4e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ],
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PFoTBBCDcVjv"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KMR1cEfJjDq1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, BatchNormalization, SeparableConv1D, Conv1D, MaxPooling1D, Embedding, Input, MaxPool1D, Dense, Embedding, Dropout,concatenate, Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, concatenate, Flatten\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HlcLXlSOWi1P"
      },
      "outputs": [],
      "source": [
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "dropout_rate = 0.5\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "num_classes = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHn0ZXCQqJ2S"
      },
      "source": [
        "# 1. Code gốc CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njBANdn5mg-B",
        "outputId": "c9f031d3-0ff0-45e9-cf91-a92e08313b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 297, 100), dtype=tf.float32, name=None), name='conv1d_1/Relu:0', description=\"created by layer 'conv1d_1'\")\n",
            "Model: \"CNN Org\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 300, 400)     1386000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 298, 100)     120100      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 297, 100)     160100      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 296, 100)     200100      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 1, 100)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3, 100)       0           ['max_pooling1d[0][0]',          \n",
            "                                                                  'max_pooling1d_1[0][0]',        \n",
            "                                                                  'max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 300)          0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 300)          0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 3)            903         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,867,203\n",
            "Trainable params: 1,867,203\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "print(conv_1)\n",
        "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(dropout_rate)(flatten)\n",
        "output = Dense(units=num_classes, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model_org = Model(inputs, output, name=\"CNN Org\")\n",
        "\n",
        "model_org.summary()\n",
        "model_infor = {}\n",
        "model_infor['CNN_Org'] = model_org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zXl-gm6qQQl"
      },
      "source": [
        "# 2. Code cải thiện CNN + BiLSTM + Attention larer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QBa1SUmqX6Q",
        "outputId": "f7ee4603-99e4-4ea6-85f7-948ef39bd6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CRNN_Enhanced\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 300, 400)     1386000     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 300, 400)     0           ['embedding[2][0]']              \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 300, 256)     541696      ['reshape_2[0][0]']              \n",
            "                                                                                                  \n",
            " separable_conv1d_4 (SeparableC  (None, 300, 100)    40900       ['reshape_2[0][0]']              \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv1d_5 (SeparableC  (None, 300, 100)    41300       ['reshape_2[0][0]']              \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv1d_6 (SeparableC  (None, 300, 100)    41700       ['reshape_2[0][0]']              \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv1d_7 (SeparableC  (None, 300, 100)    42100       ['reshape_2[0][0]']              \n",
            " onv1D)                                                                                           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 300, 1)       257         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 300, 100)    400         ['separable_conv1d_4[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 300, 100)    400         ['separable_conv1d_5[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 300, 100)    400         ['separable_conv1d_6[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 300, 100)    400         ['separable_conv1d_7[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)     (None, 300, 1)       0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 100)         0           ['batch_normalization_4[0][0]']  \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 100)         0           ['batch_normalization_5[0][0]']  \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_6 (Global  (None, 100)         0           ['batch_normalization_6[0][0]']  \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_7 (Global  (None, 100)         0           ['batch_normalization_7[0][0]']  \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 300, 256)     0           ['tf.nn.softmax[0][0]',          \n",
            "                                                                  'bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 400)          0           ['global_max_pooling1d_4[0][0]', \n",
            "                                                                  'global_max_pooling1d_5[0][0]', \n",
            "                                                                  'global_max_pooling1d_6[0][0]', \n",
            "                                                                  'global_max_pooling1d_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLambda  (None, 256)         0           ['tf.math.multiply[0][0]']       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 656)          0           ['concatenate_2[0][0]',          \n",
            "                                                                  'tf.math.reduce_sum[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 656)          0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 3)            1971        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,097,524\n",
            "Trainable params: 2,096,724\n",
            "Non-trainable params: 800\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, SpatialDropout1D, Bidirectional, Dense, Dropout\n",
        "filter_sizes = [2, 3, 4, 5]  # dùng nhiều filter hơn\n",
        "drop = 0.4\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# ===================== CNN Branch =====================\n",
        "convs = []\n",
        "for size in filter_sizes:\n",
        "    conv = SeparableConv1D(filters=num_filters,\n",
        "                           kernel_size=size,\n",
        "                           activation='relu',\n",
        "                           padding='same',\n",
        "                           kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "    conv = BatchNormalization()(conv)\n",
        "    conv = GlobalMaxPooling1D()(conv)\n",
        "    convs.append(conv)\n",
        "\n",
        "cnn_concat = concatenate(convs)\n",
        "\n",
        "# ===================== BiLSTM Branch =====================\n",
        "bilstm = Bidirectional(LSTM(128, return_sequences=True))(reshape)\n",
        "\n",
        "# Attention Layer\n",
        "def attention_layer(inputs):\n",
        "    score = Dense(1, activation='tanh')(inputs)\n",
        "    weights = tf.nn.softmax(score, axis=1)\n",
        "    context = weights * inputs\n",
        "    return tf.reduce_sum(context, axis=1)\n",
        "\n",
        "attn_output = attention_layer(bilstm)\n",
        "\n",
        "# ===================== Merge CNN + BiLSTM + Attention =====================\n",
        "merged = concatenate([cnn_concat, attn_output])\n",
        "merged = Dropout(drop)(merged)\n",
        "output = Dense(3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(merged)\n",
        "\n",
        "model_improve_cnn = Model(inputs, output, name=\"CRNN_Enhanced\")\n",
        "\n",
        "# Hiển thị kiến trúc mô hình\n",
        "model_improve_cnn.summary()\n",
        "model_infor['CRNN_Enhanced'] = model_improve_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woYCKACUsGLg"
      },
      "source": [
        "# 3. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1EdXSzE-sI_s"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM, SpatialDropout1D, Dense, Dropout\n",
        "\n",
        "embedding_dropout = SpatialDropout1D(0.2)(embedding)\n",
        "lstm = LSTM(128, return_sequences=False, dropout=dropout_rate, recurrent_dropout=dropout_rate)(embedding_dropout)\n",
        "dense = Dense(128, activation='relu')(lstm)\n",
        "drop = Dropout(dropout_rate)(dense)\n",
        "output = Dense(units=num_classes, activation='softmax')(drop)\n",
        "\n",
        "model_lstm = Model(inputs, output, name=\"LSTM_Simple\")\n",
        "model_infor['LSTM_Simple'] = model_lstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEEedY8auyK9"
      },
      "source": [
        "# Stacked LSTM (2 lớp nối tiếp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-47aT_M2u0Sf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM, SpatialDropout1D, Dense, Dropout\n",
        "\n",
        "embedding_dropout = SpatialDropout1D(0.2)(embedding)\n",
        "lstm_1 = LSTM(256, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)(embedding_dropout)\n",
        "lstm_2 = LSTM(128, return_sequences=False, dropout=dropout_rate, recurrent_dropout=dropout_rate)(lstm_1)\n",
        "dense = Dense(128, activation='relu')(lstm_2)\n",
        "drop = Dropout(dropout_rate)(dense)\n",
        "output = Dense(units=num_classes, activation='softmax')(drop)\n",
        "\n",
        "model_stacked_lstm = Model(inputs, output, name=\"Stacked_LSTM\")\n",
        "model_infor['Stacked_LSTM'] = model_stacked_lstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g34qmrCou2U_"
      },
      "source": [
        "# Song song BiLSTM đầu-cuối (Parallel BiLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9cOUtLyeu5P_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional, LSTM, concatenate\n",
        "\n",
        "embedding_dropout = SpatialDropout1D(0.2)(embedding)\n",
        "\n",
        "# BiLSTM 1\n",
        "bilstm1 = Bidirectional(LSTM(128, return_sequences=False, dropout=dropout_rate))(embedding_dropout)\n",
        "\n",
        "# BiLSTM 2\n",
        "bilstm2 = Bidirectional(LSTM(64, return_sequences=False, dropout=dropout_rate))(embedding_dropout)\n",
        "\n",
        "# Kết hợp song song\n",
        "merged = concatenate([bilstm1, bilstm2])\n",
        "dense = Dense(128, activation='relu')(merged)\n",
        "drop = Dropout(dropout_rate)(dense)\n",
        "output = Dense(units=num_classes, activation='softmax')(drop)\n",
        "\n",
        "model_parallel_bilstm = Model(inputs, output, name=\"Parallel_BiLSTM\")\n",
        "model_infor['Parallel_BiLSTM'] = model_parallel_bilstm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0piQf3Ybu8o3"
      },
      "outputs": [],
      "source": [
        "CNN + LSTM kết hợp (CNN -> LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cq-QqZrvvA7v"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Reshape\n",
        "\n",
        "# CNN\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
        "cnn = Conv1D(128, 5, activation='relu', padding='same')(reshape)\n",
        "pool = MaxPooling1D(pool_size=2)(cnn)\n",
        "\n",
        "# LSTM\n",
        "lstm = LSTM(128, return_sequences=False, dropout=dropout_rate, recurrent_dropout=dropout_rate)(pool)\n",
        "\n",
        "# Dense layers\n",
        "dense = Dense(128, activation='relu')(lstm)\n",
        "drop = Dropout(dropout_rate)(dense)\n",
        "output = Dense(units=num_classes, activation='softmax')(drop)\n",
        "\n",
        "model_cnn_lstm = Model(inputs, output, name=\"CNN_LSTM\")\n",
        "model_infor['CNN_LSTM'] = model_cnn_lstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzFgSIkQsrfz"
      },
      "source": [
        "# 4. CNN +BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozXhkkuBg9or",
        "outputId": "42b6d3b5-c0ff-40fe-c8ee-275354e7f29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CNN_BiLSTM\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 300, 400)     1386000     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 300, 128)     153728      ['embedding[2][0]']              \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 300, 128)     204928      ['embedding[2][0]']              \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 300, 128)     256128      ['embedding[2][0]']              \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, 256)         541696      ['embedding[2][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_8 (Global  (None, 128)         0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_9 (Global  (None, 128)         0           ['conv1d_5[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_10 (Globa  (None, 128)         0           ['conv1d_6[0][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 640)          0           ['bidirectional_3[0][0]',        \n",
            "                                                                  'global_max_pooling1d_8[0][0]', \n",
            "                                                                  'global_max_pooling1d_9[0][0]', \n",
            "                                                                  'global_max_pooling1d_10[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 640)         2560        ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 640)          0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 128)          82048       ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 128)          0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 3)            387         ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,627,475\n",
            "Trainable params: 2,626,195\n",
            "Non-trainable params: 1,280\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, concatenate\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, BatchNormalization\n",
        "\n",
        "# Multi-kernel CNN\n",
        "conv_0 = Conv1D(filters=128,\n",
        "                kernel_size=3,\n",
        "                padding='same',\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_1 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "conv_2 = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
        "\n",
        "# MaxPooling\n",
        "maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
        "maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
        "maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
        "\n",
        "# BiLSTM\n",
        "bilstm = Bidirectional(LSTM(128, return_sequences=False))(embedding)\n",
        "\n",
        "# Kết hợp BiLSTM và CNN\n",
        "merged = concatenate([bilstm, maxpool_0, maxpool_1, maxpool_2])\n",
        "\n",
        "# Thêm BatchNormalization và Dropout\n",
        "x = BatchNormalization()(merged)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "\n",
        "# Dense layer trung gian giúp học tốt hơn\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "\n",
        "# Output\n",
        "output = Dense(units=3, activation='softmax')(x)\n",
        "\n",
        "# Model\n",
        "model_cnn_bilstm = Model(inputs, output, name=\"CNN_BiLSTM\")\n",
        "model_cnn_bilstm.summary()\n",
        "model_infor['CNN_BiLSTM'] = model_cnn_bilstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDO5FzTg1rcx"
      },
      "source": [
        "# 5. LSTMs + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWiCzXce1ox7",
        "outputId": "e3a01a19-31d0-4f0e-d204-4350729858d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"LSTM_CNN\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 300, 400)     1386000     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 300, 256)    541696      ['embedding[2][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 300, 128)     98432       ['bidirectional_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 300, 128)     131200      ['bidirectional_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 300, 128)     163968      ['bidirectional_4[0][0]']        \n",
            "                                                                                                  \n",
            " global_max_pooling1d_11 (Globa  (None, 128)         0           ['conv1d_7[0][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_12 (Globa  (None, 128)         0           ['conv1d_8[0][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " global_max_pooling1d_13 (Globa  (None, 128)         0           ['conv1d_9[0][0]']               \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 384)          0           ['global_max_pooling1d_11[0][0]',\n",
            "                                                                  'global_max_pooling1d_12[0][0]',\n",
            "                                                                  'global_max_pooling1d_13[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 384)         1536        ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 384)          0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 128)          49280       ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 128)          0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 3)            387         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,372,499\n",
            "Trainable params: 2,371,731\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, concatenate\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_lstm_cnn_model(inputs,\n",
        "                         embedding,\n",
        "                         lstm_units=128,\n",
        "                         cnn_filters=128,\n",
        "                         dropout_rate=0.5,\n",
        "                         num_classes=3):\n",
        "    \"\"\"\n",
        "    Xây dựng mô hình với kiến trúc Embedding -> BiLSTM -> CNN.\n",
        "\n",
        "    Args:\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.keras.models.Model: Mô hình Keras.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # BiLSTM\n",
        "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding) # return_sequences=True cho CNN sau đó\n",
        "\n",
        "    # Multi-kernel CNN\n",
        "    conv_0 = Conv1D(filters=cnn_filters,\n",
        "                    kernel_size=3,\n",
        "                    padding='same',\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(0.01))(bilstm)\n",
        "    conv_1 = Conv1D(filters=cnn_filters,\n",
        "                    kernel_size=4, padding='same',\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(0.01))(bilstm)\n",
        "    conv_2 = Conv1D(filters=cnn_filters, kernel_size=5,\n",
        "                    padding='same',\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(0.01))(bilstm)\n",
        "\n",
        "    # MaxPooling\n",
        "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
        "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
        "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
        "\n",
        "    # Kết hợp BiLSTM và CNN\n",
        "    merged = concatenate([maxpool_0, maxpool_1, maxpool_2]) # BiLSTM đã được xử lý bởi CNN\n",
        "\n",
        "    # Thêm BatchNormalization và Dropout\n",
        "    x = BatchNormalization()(merged)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = Dense(lstm_units, activation='relu')(x) # Giữ số units tương đương LSTM\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output\n",
        "    output = Dense(units=num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Model\n",
        "    model_lstm_cnn = Model(inputs, output, name=\"LSTM_CNN\")\n",
        "    return model_lstm_cnn\n",
        "\n",
        "# Tạo và tóm tắt mô hình\n",
        "model_lstm_cnn = build_lstm_cnn_model(\n",
        "    inputs,\n",
        "    embedding,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=3 # Thay đổi nếu số lượng lớp của bạn khác\n",
        ")\n",
        "model_lstm_cnn.summary()\n",
        "\n",
        "# Lưu thông tin mô hình (nếu bạn đang sử dụng dictionary model_infor)\n",
        "if 'model_infor' in locals():\n",
        "    model_infor['LSTM_CNN'] = model_lstm_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTDuPWlzUoXL"
      },
      "source": [
        "# 6. CRNN code gốc của thầy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mlNNL33UM39",
        "outputId": "8513467a-e3ad-4013-fcd2-fb06de3f7e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CRNN\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 300, 400)     1386000     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 300, 400)     0           ['embedding[3][0]']              \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 300, 100)     120100      ['reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 300, 100)     160100      ['reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 300, 100)     200100      ['reshape_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 1, 300)       0           ['max_pooling1d_4[0][0]',        \n",
            "                                                                  'max_pooling1d_5[0][0]',        \n",
            "                                                                  'max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 300)          0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 3)            903         ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,867,203\n",
            "Trainable params: 1,867,203\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "################## LSTM ONLY ###############################\n",
        "# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "################# SINGLE LSTM ####################\n",
        "# lstm_0 = LSTM(512)(reshape)\n",
        "\n",
        "# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "\n",
        "################## CRNN ####################################\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
        "\n",
        "conv_0 = Conv1D(num_filters,\n",
        " (filter_sizes[0], ),\n",
        "                padding=\"same\",\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv1D(num_filters,\n",
        " (filter_sizes[1], ),\n",
        "                padding=\"same\",\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv1D(num_filters,\n",
        " (filter_sizes[2], ),\n",
        "                padding=\"same\",\n",
        "                activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "conv_0 = MaxPool1D(300)(conv_0)\n",
        "conv_1 = MaxPool1D(300)(conv_1)\n",
        "conv_2 = MaxPool1D(300)(conv_2)\n",
        "# Reshape output to match RNN dimension\n",
        "# conv_0 = Reshape((-1, num_filters))(conv_0)\n",
        "# conv_1 = Reshape((-1, num_filters))(conv_1)\n",
        "# conv_2 = Reshape((-1, num_filters))(conv_2)\n",
        "\n",
        "concat = concatenate([conv_0, conv_1, conv_2])\n",
        "concat = Flatten()(concat)\n",
        "# lstm_0 = LSTM(512)(concat)\n",
        "\n",
        "# YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n",
        "# lstm_2 = LSTM(1024, return_sequences=True)(concat)\n",
        "# lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n",
        "# lstm_0 = LSTM(256)(lstm_1)\n",
        "\n",
        "############################################################\n",
        "\n",
        "dropout = Dropout(drop)(concat)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "# this creates a model that includes\n",
        "model_crnn = Model(inputs, output, name=\"CRNN\")\n",
        "model_crnn.summary()\n",
        "model_infor['CRNN'] = model_crnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IguuM6iJvVZ5"
      },
      "source": [
        "# CRNN (CNN + BiLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Cp07kpHXvZ9m"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional, LSTM\n",
        "\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# CNN với nhiều filter size\n",
        "conv_blocks = []\n",
        "for size in filter_sizes:\n",
        "    conv = Conv1D(num_filters, kernel_size=size, padding=\"same\", activation='relu',\n",
        "                  kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "    conv = MaxPool1D(pool_size=2)(conv)\n",
        "    conv_blocks.append(conv)\n",
        "\n",
        "cnn_concat = concatenate(conv_blocks)\n",
        "\n",
        "# BiLSTM sau CNN\n",
        "bilstm = Bidirectional(LSTM(128, return_sequences=False))(cnn_concat)\n",
        "\n",
        "dense = Dense(128, activation='relu')(bilstm)\n",
        "drop = Dropout(drop)(dense)\n",
        "output = Dense(units=3, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(drop)\n",
        "\n",
        "model_crnn_improved = Model(inputs, output, name=\"CRNN_Improved\")\n",
        "model_infor['CRNN_Improved'] = model_crnn_improved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rssMZEd0vd6U"
      },
      "source": [
        "# RNN + CNN + LSTM (theo thứ tự RNN → CNN → LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oSukGh_Cvc5k"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# RNN đơn trước\n",
        "rnn = SimpleRNN(128, return_sequences=True)(reshape)\n",
        "\n",
        "# CNN tiếp theo\n",
        "conv = Conv1D(num_filters, 5, activation='relu', padding='same')(rnn)\n",
        "pool = MaxPool1D(pool_size=2)(conv)\n",
        "\n",
        "# LSTM cuối cùng\n",
        "lstm = LSTM(128, return_sequences=False)(pool)\n",
        "\n",
        "dense = Dense(128, activation='relu')(lstm)\n",
        "drop = Dropout(drop)(dense)\n",
        "output = Dense(units=3, activation='softmax')(drop)\n",
        "\n",
        "model_rnn_cnn_lstm = Model(inputs, output, name=\"RNN_CNN_LSTM\")\n",
        "model_infor['RNN_CNN_LSTM'] = model_rnn_cnn_lstm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u_8WfyAvlFl"
      },
      "source": [
        "# LSTM + RNN song song (LSTM & RNN chạy song song rồi nối lại)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VpDWUKm_vm0O"
      },
      "outputs": [],
      "source": [
        "reshape = Reshape((sequence_length, EMBEDDING_DIM))(embedding)\n",
        "\n",
        "# LSTM\n",
        "lstm_branch = LSTM(128, return_sequences=False)(reshape)\n",
        "\n",
        "# RNN\n",
        "rnn_branch = SimpleRNN(128, return_sequences=False)(reshape)\n",
        "\n",
        "# Ghép song song\n",
        "merged = concatenate([lstm_branch, rnn_branch])\n",
        "dense = Dense(128, activation='relu')(merged)\n",
        "drop = Dropout(drop)(dense)\n",
        "output = Dense(units=3, activation='softmax')(drop)\n",
        "\n",
        "model_lstm_rnn = Model(inputs, output, name=\"LSTM_RNN_Parallel\")\n",
        "model_infor['LSTM_RNN_Parallel'] = model_lstm_rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_e_cxIstXbX"
      },
      "source": [
        "# 7. BERT-based Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "_n76xzFEtcbR",
        "outputId": "5800bafc-8e95-4674-b614-52406f5a445f"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-bb66b353ccea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Thêm import os\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# <<< THAY ĐỔI CÁC THAM SỐ NÀY NẾU CẦN >>>\n",
        "MODEL_NAME = 'vinai/phobert-base' # Hoặc 'vinai/phobert-large' nếu có GPU mạnh\n",
        "MAX_LEN = 128      # Độ dài tối đa sequence (phù hợp với PhoBERT)\n",
        "BATCH_SIZE = 16     # Giảm nếu gặp lỗi Out-of-Memory (OOM) trên GPU\n",
        "EPOCHS = 4          # Số epochs huấn luyện (thường 2-5 là đủ cho fine-tuning)\n",
        "LEARNING_RATE = 2e-5 # Learning rate phổ biến cho fine-tuning BERT\n",
        "NUM_LABELS = 3      # Số lượng nhãn sentiment (VD: 0: Tiêu cực, 1: Trung tính, 2: Tích cực)\n",
        "RANDOM_SEED = 42\n",
        "OUTPUT_DIR = './bert_sentiment_model/' # Thư mục lưu model tốt nhất\n",
        "\n",
        "# --- Đường dẫn dữ liệu (thay thế bằng đường dẫn thực tế) ---\n",
        "# Giả sử file CSV có cột 'text' và 'label' (label là số nguyên 0, 1, 2...)\n",
        "TRAIN_DATA_PATH = 'path/to/your/sentiment_data.csv' # <<< THAY ĐỔI ĐƯỜNG DẪN NÀY\n",
        "TEXT_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "# TEST_DATA_PATH = 'path/to/your/test.csv' # Nếu có file test riêng\n",
        "\n",
        "# --- Thiết lập seed để có thể tái tạo kết quả ---\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    # Cấu hình thêm để đảm bảo tính xác định (có thể làm chậm quá trình huấn luyện)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- Thiết lập device (GPU nếu có) ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU')\n",
        "\n",
        "# --- 2. Load Data ---\n",
        "print(f\"\\nLoading data from: {TRAIN_DATA_PATH}\")\n",
        "try:\n",
        "    df = pd.read_csv(TRAIN_DATA_PATH)\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nValue counts for labels ({LABEL_COLUMN}):\")\n",
        "    print(df[LABEL_COLUMN].value_counts())\n",
        "\n",
        "    # Kiểm tra số lượng nhãn thực tế\n",
        "    actual_num_labels = df[LABEL_COLUMN].nunique()\n",
        "    if actual_num_labels != NUM_LABELS:\n",
        "        print(f\"\\n*** Warning: NUM_LABELS is set to {NUM_LABELS}, but found {actual_num_labels} unique labels in the data. Please check your configuration and data.\")\n",
        "        # NUM_LABELS = actual_num_labels # Cập nhật nếu muốn tự động điều chỉnh\n",
        "\n",
        "    # Đảm bảo cột text là string và xử lý NaN\n",
        "    df[TEXT_COLUMN] = df[TEXT_COLUMN].astype(str).fillna('')\n",
        "    df = df[[TEXT_COLUMN, LABEL_COLUMN]] # Chỉ giữ lại cột cần thiết\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {TRAIN_DATA_PATH}\")\n",
        "    exit()\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Column '{e}' not found in the CSV file. Ensure your CSV has columns named '{TEXT_COLUMN}' and '{LABEL_COLUMN}'.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during data loading: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Chia dữ liệu thành train và validation (chưa dùng test set ở đây)\n",
        "print(\"\\nSplitting data into training and validation sets...\")\n",
        "df_train, df_val = train_test_split(\n",
        "    df,\n",
        "    test_size=0.1, # 10% cho validation\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=df[LABEL_COLUMN] # Giữ tỷ lệ nhãn trong cả 2 tập\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(df_train)}\")\n",
        "print(f\"Validation size: {len(df_val)}\")\n",
        "# print(f\"Test size: {len(df_test)}\") # Nếu có\n",
        "\n",
        "# --- 3. Tokenizer & Dataset ---\n",
        "print(f\"\\nLoading Tokenizer for {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True, # Thêm '[CLS]' và '[SEP]' (hoặc '<s>', '</s>' cho PhoBERT)\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False, # Không cần cho sequence classification với 1 câu\n",
        "            padding='max_length', # Pad đến max_length\n",
        "            truncation=True,      # Cắt nếu dài hơn max_length\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',  # Trả về PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size, num_workers=2):\n",
        "    ds = SentimentDataset(\n",
        "        texts=df[TEXT_COLUMN].to_numpy(),\n",
        "        labels=df[LABEL_COLUMN].to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers, # Số worker để load data song song\n",
        "        shuffle=True # Shuffle dữ liệu trong quá trình huấn luyện\n",
        "    )\n",
        "\n",
        "# Tạo DataLoader cho train và validation\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "# Không cần shuffle validation loader\n",
        "val_dataset = SentimentDataset(\n",
        "    texts=df_val[TEXT_COLUMN].to_numpy(),\n",
        "    labels=df_val[LABEL_COLUMN].to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
        "\n",
        "# test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE) # Nếu có test set\n",
        "\n",
        "# --- 4. Model ---\n",
        "print(f\"\\nLoading Pre-trained Model: {MODEL_NAME} for Sequence Classification...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    output_attentions=False, # Không cần attention weights khi fine-tuning\n",
        "    output_hidden_states=False, # Không cần hidden states khi fine-tuning\n",
        ")\n",
        "model.to(device)\n",
        "print(\"Model loaded and moved to device.\")\n",
        "\n",
        "# --- 5. Optimizer & Scheduler ---\n",
        "# AdamW là optimizer thường dùng cho BERT\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS # Tổng số bước huấn luyện\n",
        "\n",
        "# Scheduler để giảm learning rate theo thời gian\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0, # Số bước \"khởi động\" (thường là 0 hoặc ~10% total_steps)\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# --- 6. Training & Evaluation Functions ---\n",
        "def format_time(elapsed):\n",
        "    '''Chuyển đổi thời gian (giây) sang định dạng hh:mm:ss'''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, epoch_num, total_epochs):\n",
        "    \"\"\"Huấn luyện một epoch.\"\"\"\n",
        "    print(f'\\n======== Epoch {epoch_num + 1} / {total_epochs} ========')\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train() # Chuyển model sang chế độ training\n",
        "\n",
        "    predictions_train, true_labels_train = [], []\n",
        "\n",
        "    for step, batch in enumerate(data_loader):\n",
        "        # Log tiến trình mỗi 50 steps\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f'  Batch {step:>5,}  of  {len(data_loader):>5,}.    Elapsed: {elapsed}.')\n",
        "\n",
        "        # Đưa batch lên device (GPU/CPU)\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_attention_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad() # Xóa gradient từ bước trước\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=b_input_ids,\n",
        "            token_type_ids=None, # BERT không cần cái này cho single sequence tasks\n",
        "            attention_mask=b_attention_mask,\n",
        "            labels=b_labels # Model tự tính loss khi có labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass để tính gradient\n",
        "        loss.backward()\n",
        "\n",
        "        # Chống gradient exploding (gradient clipping)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Cập nhật weights\n",
        "        optimizer.step()\n",
        "        # Cập nhật learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Lưu lại dự đoán và nhãn thật để tính accuracy/f1 cuối epoch\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions_train.extend(np.argmax(logits, axis=1).flatten())\n",
        "        true_labels_train.extend(label_ids.flatten())\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / len(data_loader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    # Tính accuracy và F1 score cho epoch này\n",
        "    train_accuracy = accuracy_score(true_labels_train, predictions_train)\n",
        "    train_f1 = f1_score(true_labels_train, predictions_train, average='weighted') # 'weighted' cho multi-class\n",
        "\n",
        "    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Training F1 Score: {train_f1:.4f}\")\n",
        "    print(f\"  Training epoch took: {training_time}\")\n",
        "\n",
        "    return avg_train_loss, train_accuracy, train_f1\n",
        "\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"Đánh giá model trên tập dữ liệu (validation hoặc test).\"\"\"\n",
        "    print(\"\\nRunning Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval() # Chuyển model sang chế độ evaluation\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad(): # Không cần tính gradient khi đánh giá\n",
        "        for batch in data_loader:\n",
        "            b_input_ids = batch['input_ids'].to(device)\n",
        "            b_attention_mask = batch['attention_mask'].to(device)\n",
        "            b_labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=b_input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=b_attention_mask,\n",
        "                labels=b_labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Chuyển logits và labels về CPU để tính metrics\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "            true_labels.extend(label_ids.flatten())\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(data_loader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    # Tính accuracy và F1 score\n",
        "    val_accuracy = accuracy_score(true_labels, predictions)\n",
        "    val_f1 = f1_score(true_labels, predictions, average='weighted') # Hoặc 'macro'\n",
        "\n",
        "    print(f\"\\n  Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"  Validation F1 Score: {val_f1:.4f}\")\n",
        "    print(f\"  Validation took: {validation_time}\")\n",
        "\n",
        "    # In báo cáo chi tiết hơn\n",
        "    print(\"\\nClassification Report (Validation):\")\n",
        "    target_names = [f'Label_{i}' for i in range(NUM_LABELS)] # Đặt tên nhãn nếu biết\n",
        "    print(classification_report(true_labels, predictions, target_names=target_names, digits=4))\n",
        "\n",
        "    print(\"\\nConfusion Matrix (Validation):\")\n",
        "    print(confusion_matrix(true_labels, predictions))\n",
        "\n",
        "    return avg_val_loss, val_accuracy, val_f1\n",
        "\n",
        "\n",
        "# --- 7. Training Loop ---\n",
        "history = {'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
        "           'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "best_val_accuracy = 0\n",
        "total_t0 = time.time()\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "for epoch_i in range(0, EPOCHS):\n",
        "\n",
        "    # --- Training ---\n",
        "    avg_train_loss, train_accuracy, train_f1 = train_epoch(\n",
        "        model, train_data_loader, optimizer, device, scheduler, epoch_i, EPOCHS\n",
        "    )\n",
        "\n",
        "    # --- Validation ---\n",
        "    avg_val_loss, val_accuracy, val_f1 = eval_model(\n",
        "        model, val_data_loader, device\n",
        "    )\n",
        "\n",
        "    # Lưu lại lịch sử\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['train_acc'].append(train_accuracy)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_acc'].append(val_accuracy)\n",
        "    history['val_f1'].append(val_f1)\n",
        "\n",
        "    # Lưu model tốt nhất dựa trên validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        print(f\"\\n  Saving best model to {OUTPUT_DIR} (Epoch {epoch_i + 1})\")\n",
        "\n",
        "        # Tạo thư mục nếu chưa có\n",
        "        if not os.path.exists(OUTPUT_DIR):\n",
        "            os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "        # Lưu model và tokenizer\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model # Xử lý khi dùng DataParallel\n",
        "        model_to_save.save_pretrained(OUTPUT_DIR)\n",
        "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "        # Có thể lưu thêm các tham số khác nếu cần\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "\n",
        "print(\"\\n--- Training complete! ---\")\n",
        "print(f\"Total training took {format_time(time.time()-total_t0)}\")\n",
        "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# --- 9. Plot learning curves ---\n",
        "def plot_bert_history(history, model_name=\"BERT\"):\n",
        "    \"\"\"Vẽ đồ thị accuracy, F1 và loss trong quá trình huấn luyện BERT.\"\"\"\n",
        "    epochs_range = range(1, len(history['train_acc']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'{model_name} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # F1 Score plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs_range, history['train_f1'], label='Training F1 Score')\n",
        "    plt.plot(epochs_range, history['val_f1'], label='Validation F1 Score')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(f'{model_name} - F1 Score (Weighted)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(f'{model_name} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.suptitle(f'Learning Curves for {model_name}', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nPlotting learning curves...\")\n",
        "plot_bert_history(history, model_name=MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvp42YstUho"
      },
      "source": [
        "# 4. Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TXnFyHH1aWlE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwsgrjBata7e",
        "outputId": "0eae7404-6c36-4225-bd5b-44ea711a80ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model 'CNN_Org'...\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 29s 6s/step - loss: 4.8017 - accuracy: 0.8083 - val_loss: 6.6625 - val_accuracy: 0.1048\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 18s 4s/step - loss: 4.4529 - accuracy: 0.8726 - val_loss: 7.5935 - val_accuracy: 0.0714\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 19s 4s/step - loss: 4.1265 - accuracy: 0.9119 - val_loss: 6.4488 - val_accuracy: 0.1238\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 19s 4s/step - loss: 3.9652 - accuracy: 0.9298 - val_loss: 6.6837 - val_accuracy: 0.1190\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 17s 4s/step - loss: 3.7657 - accuracy: 0.9357 - val_loss: 6.9113 - val_accuracy: 0.0524\n",
            "Training model 'CRNN_Enhanced'...\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 57s 14s/step - loss: 4.0792 - accuracy: 0.3988 - val_loss: 1.4148 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 54s 13s/step - loss: 3.0562 - accuracy: 0.4405 - val_loss: 1.4315 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "2/4 [==============>...............] - ETA: 26s - loss: 2.1826 - accuracy: 0.5527"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7b9d88e5f0d0>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/backend.py\", line 5159, in <genexpr>\n",
            "    output_ta_t = tuple(  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 70s 19s/step - loss: 2.2163 - accuracy: 0.5690 - val_loss: 1.5458 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.8432 - accuracy: 0.6095 - val_loss: 1.3278 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 52s 13s/step - loss: 1.4466 - accuracy: 0.6714 - val_loss: 1.3457 - val_accuracy: 0.0000e+00\n",
            "Training model 'LSTM_Simple'...\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 49s 12s/step - loss: 1.1000 - accuracy: 0.3619 - val_loss: 1.5697 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 50s 12s/step - loss: 1.0495 - accuracy: 0.4429 - val_loss: 1.8151 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 49s 12s/step - loss: 1.0171 - accuracy: 0.4810 - val_loss: 1.9706 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 56s 14s/step - loss: 1.0299 - accuracy: 0.4405 - val_loss: 2.0208 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 51s 13s/step - loss: 0.9981 - accuracy: 0.5048 - val_loss: 2.0342 - val_accuracy: 0.0000e+00\n",
            "Epoch 5: early stopping\n",
            "Training model 'Stacked_LSTM'...\n",
            "Epoch 1/5\n",
            "1/4 [======>.......................] - ETA: 2:28 - loss: 1.1034 - accuracy: 0.3516"
          ]
        }
      ],
      "source": [
        "# Compile models\n",
        "for name, model in model_infor.items():\n",
        "    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train models\n",
        "history = {}\n",
        "for name, model in model_infor.items():\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
        "    callbacks_list = [early_stopping]\n",
        "    print(f\"Training model '{name}'...\")\n",
        "    history[name] = model.fit(data, labels,\n",
        "                               validation_split=0.2,\n",
        "                               epochs=5,\n",
        "                               batch_size=256,\n",
        "                               callbacks=callbacks_list,\n",
        "                               shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_wzXkJjPmg5"
      },
      "source": [
        "# Đánh giá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XoN2UOamg-D"
      },
      "outputs": [],
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwiYb3Ohmg-E"
      },
      "outputs": [],
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E08tBw9img-E"
      },
      "outputs": [],
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = []\n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwgI9Xywmg-E"
      },
      "outputs": [],
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p02GxCh6mg-F"
      },
      "outputs": [],
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAqUMGInmg-F"
      },
      "outputs": [],
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrYmjfq8QqpS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def compare_models_performance(history_dict, model_dict, X_test, y_test):\n",
        "    \"\"\"\n",
        "    So sánh hiệu suất của các model bao gồm:\n",
        "    - Validation accuracy, validation loss theo epochs\n",
        "    - Accuracy, loss, precision, recall trên tập test\n",
        "    - Các biểu đồ trực quan hóa\n",
        "\n",
        "    Args:\n",
        "        history_dict: dict chứa lịch sử huấn luyện (tên -> history)\n",
        "        model_dict: dict chứa models (tên -> model)\n",
        "        X_test: dữ liệu kiểm thử\n",
        "        y_test: nhãn kiểm thử\n",
        "    \"\"\"\n",
        "    # 3. Đánh giá test set\n",
        "    test_metrics = {}\n",
        "\n",
        "    print(\"\\nMODEL TEST PERFORMANCE\")\n",
        "    print(\"=\" * 50)\n",
        "    for name, model in model_dict.items():\n",
        "        results = model.evaluate(X_test, y_test, verbose=0)\n",
        "        metrics_names = model.metrics_names\n",
        "        test_metrics[name] = dict(zip(metrics_names, results))\n",
        "\n",
        "        metric_str = ' | '.join([\n",
        "            f\"{k.capitalize()}: {v*100:.2f}%\" if k != 'loss' else f\"Loss: {v:.4f}\"\n",
        "            for k, v in test_metrics[name].items()\n",
        "        ])\n",
        "        print(f\"{name:20s} -> {metric_str}\")\n",
        "\n",
        "    # 4. Bar chart: Test Accuracy (%)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    acc_values = [test_metrics[name].get('accuracy', 0) * 100 for name in model_dict.keys()]\n",
        "    plt.bar(model_dict.keys(), acc_values, color='skyblue')\n",
        "    plt.title('Test Accuracy Comparison')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid(axis='y')\n",
        "    for i, acc in enumerate(acc_values):\n",
        "        plt.text(i, acc / 2, f'{acc:.2f}%', ha='center', color='black', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Bar chart: Test Loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    loss_values = [test_metrics[name].get('loss', 0) for name in model_dict.keys()]\n",
        "    plt.bar(model_dict.keys(), loss_values, color='salmon')\n",
        "    plt.title('Test Loss Comparison')\n",
        "    plt.ylabel('Loss %')\n",
        "    plt.grid(axis='y')\n",
        "    for i, loss in enumerate(loss_values):\n",
        "        plt.text(i, loss / 2, f'{loss:.2f}%', ha='center', color='black', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "test_accuracies = compare_models_performance(history, model_infor, data_test, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r31_uxxgmg-G"
      },
      "outputs": [],
      "source": [
        "for name, model in model_infor.items():\n",
        "  print(name)\n",
        "  score = model.evaluate( data_test, labels_test)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "thanhkieu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
